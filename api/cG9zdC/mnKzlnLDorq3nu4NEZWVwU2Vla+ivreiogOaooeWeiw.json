{"title":"本地训练 DeepSeek 语言模型","date":"2025-02-20T00:41:47.798Z","slug":"本地训练DeepSeek语言模型","comments":true,"tags":["DeepSeek","人工智能","大模型","模型训练"],"updated":"2025-02-20T00:46:51.417Z","content":"<h2 id=\"1-硬件与环境配置\">1. 硬件与环境配置<a href=\"post/本地训练DeepSeek语言模型#1-硬件与环境配置\"></a></h2><h3 id=\"硬件要求\">硬件要求<a href=\"post/本地训练DeepSeek语言模型#硬件要求\"></a></h3><ul>\n<li><strong>显存</strong>：A100 (80GB) 或 4090 (24GB) 及以上</li>\n<li><strong>CPU</strong>：64 核以上，建议 AMD EPYC 或 Intel Xeon</li>\n<li><strong>内存</strong>：至少 128GB RAM</li>\n<li><strong>存储</strong>：NVMe SSD 2TB+，用于存储数据集和模型权重</li>\n</ul>\n<h3 id=\"安装所需依赖\">安装所需依赖<a href=\"post/本地训练DeepSeek语言模型#安装所需依赖\"></a></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</span><br><span class=\"line\">pip install transformers datasets accelerate peft deepspeed bitsandbytes</span><br></pre></td></tr></table></figure>\n<h2 id=\"2-下载-DeepSeek-语言模型\">2. 下载 DeepSeek 语言模型<a href=\"post/本地训练DeepSeek语言模型#2-下载-DeepSeek-语言模型\"></a></h2><p>DeepSeek 提供多个预训练语言模型，常用的是 <strong>DeepSeek-LLM 7B/67B</strong>，可以从 Hugging Face 下载：<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">huggingface-cli login</span><br><span class=\"line\">git lfs install</span><br><span class=\"line\">git <span class=\"built_in\">clone</span> https://huggingface.co/deepseek-ai/deepseek-llm-7b-base</span><br></pre></td></tr></table></figure></p>\n<p>或在代码中加载：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class=\"line\"></span><br><span class=\"line\">model_name = <span class=\"string\">\"deepseek-ai/deepseek-llm-7b-base\"</span></span><br><span class=\"line\">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class=\"line\">model = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class=\"string\">\"auto\"</span>, torch_dtype=<span class=\"string\">\"auto\"</span>)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"3-数据准备\">3. 数据准备<a href=\"post/本地训练DeepSeek语言模型#3-数据准备\"></a></h2><h3 id=\"开源数据\">开源数据<a href=\"post/本地训练DeepSeek语言模型#开源数据\"></a></h3><ul>\n<li><strong>WuDaoCorpus</strong>（悟道语料）</li>\n<li><strong>Chinese Wikipedia</strong></li>\n<li><strong>C4-Zh</strong>（清洗后的中文 C4 语料）</li>\n<li><strong>CLUECorpus2023</strong>（清华 NLP 提供）</li>\n</ul>\n<h3 id=\"数据格式\">数据格式<a href=\"post/本地训练DeepSeek语言模型#数据格式\"></a></h3><p>DeepSeek 训练数据需为 JSON 格式：<br><figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">  &#123;<span class=\"attr\">\"text\"</span>: <span class=\"string\">\"人类的语言是如何进化的？\"</span>&#125;,</span><br><span class=\"line\">  &#123;<span class=\"attr\">\"text\"</span>: <span class=\"string\">\"机器学习的基本原理是什么？\"</span>&#125;</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure></p>\n<p><strong>加载数据示例</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> datasets <span class=\"keyword\">import</span> load_dataset</span><br><span class=\"line\">dataset = load_dataset(<span class=\"string\">\"json\"</span>, data_files=<span class=\"string\">\"your_dataset.json\"</span>)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"4-选择微调方法\">4. 选择微调方法<a href=\"post/本地训练DeepSeek语言模型#4-选择微调方法\"></a></h2><ul>\n<li><strong>全参数微调（Full Fine-tuning）</strong>：适用于大规模训练，但显存需求极高</li>\n<li><strong>LoRA 微调</strong>（Low-Rank Adaptation）：适用于 24GB 及以下显存</li>\n<li><strong>QLoRA 量化微调</strong>（8-bit/4-bit 量化）：适用于消费级 GPU</li>\n</ul>\n<h2 id=\"5-LoRA-低成本微调\">5. LoRA 低成本微调<a href=\"post/本地训练DeepSeek语言模型#5-LoRA-低成本微调\"></a></h2><p>LoRA 适用于 <strong>消费级 GPU</strong>，仅更新部分权重，降低显存需求：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> peft <span class=\"keyword\">import</span> LoraConfig, get_peft_model, TaskType</span><br><span class=\"line\"></span><br><span class=\"line\">config = LoraConfig(</span><br><span class=\"line\">    task_type=TaskType.CAUSAL_LM,</span><br><span class=\"line\">    inference_mode=<span class=\"literal\">False</span>,</span><br><span class=\"line\">    r=<span class=\"number\">16</span>, lora_alpha=<span class=\"number\">32</span>, lora_dropout=<span class=\"number\">0.1</span></span><br><span class=\"line\">)</span><br><span class=\"line\">model = get_peft_model(model, config)</span><br></pre></td></tr></table></figure></p>\n<p><strong>训练参数</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> TrainingArguments, Trainer</span><br><span class=\"line\"></span><br><span class=\"line\">training_args = TrainingArguments(</span><br><span class=\"line\">    output_dir=<span class=\"string\">\"./deepseek-lora\"</span>,</span><br><span class=\"line\">    per_device_train_batch_size=<span class=\"number\">2</span>,</span><br><span class=\"line\">    gradient_accumulation_steps=<span class=\"number\">8</span>,</span><br><span class=\"line\">    learning_rate=<span class=\"number\">2e-4</span>,</span><br><span class=\"line\">    num_train_epochs=<span class=\"number\">3</span>,</span><br><span class=\"line\">    save_steps=<span class=\"number\">500</span>,</span><br><span class=\"line\">    save_total_limit=<span class=\"number\">2</span>,</span><br><span class=\"line\">    logging_dir=<span class=\"string\">\"./logs\"</span>,</span><br><span class=\"line\">    fp16=<span class=\"literal\">True</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">trainer = Trainer(</span><br><span class=\"line\">    model=model,</span><br><span class=\"line\">    args=training_args,</span><br><span class=\"line\">    train_dataset=dataset[<span class=\"string\">\"train\"</span>],</span><br><span class=\"line\">    tokenizer=tokenizer</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">trainer.train()</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"6-QLoRA-低显存优化\">6. QLoRA 低显存优化<a href=\"post/本地训练DeepSeek语言模型#6-QLoRA-低显存优化\"></a></h2><p>如果显存不足，可以使用 <strong>QLoRA（4-bit 量化）</strong>：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> BitsAndBytesConfig</span><br><span class=\"line\"></span><br><span class=\"line\">bnb_config = BitsAndBytesConfig(</span><br><span class=\"line\">    load_in_4bit=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    bnb_4bit_compute_dtype=torch.float16,</span><br><span class=\"line\">    bnb_4bit_use_double_quant=<span class=\"literal\">True</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">model = AutoModelForCausalLM.from_pretrained(</span><br><span class=\"line\">    model_name, quantization_config=bnb_config, device_map=<span class=\"string\">\"auto\"</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"7-训练后推理\">7. 训练后推理<a href=\"post/本地训练DeepSeek语言模型#7-训练后推理\"></a></h2><p>训练完成后，可以加载模型进行推理：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> pipeline</span><br><span class=\"line\"></span><br><span class=\"line\">pipe = pipeline(<span class=\"string\">\"text-generation\"</span>, model=model, tokenizer=tokenizer)</span><br><span class=\"line\">result = pipe(<span class=\"string\">\"介绍一下黑洞的原理。\"</span>)</span><br><span class=\"line\">print(result[<span class=\"number\">0</span>][<span class=\"string\">\"generated_text\"</span>])</span><br></pre></td></tr></table></figure></p>\n","next":{"title":"如何将 Hugging Face 模型转换并加载到 Ollama（Windows 版）","slug":"如何将HuggingFace模型转换并加载到Ollama_Windows版"},"link":"https://www.alipay.one/post/本地训练DeepSeek语言模型/","toc":[{"title":"1. 硬件与环境配置","id":"1-硬件与环境配置","index":"1","children":[{"title":"硬件要求","id":"硬件要求","index":"1.1"},{"title":"安装所需依赖","id":"安装所需依赖","index":"1.2"}]},{"title":"2. 下载 DeepSeek 语言模型","id":"2-下载-DeepSeek-语言模型","index":"2"},{"title":"3. 数据准备","id":"3-数据准备","index":"3","children":[{"title":"开源数据","id":"开源数据","index":"3.1"},{"title":"数据格式","id":"数据格式","index":"3.2"}]},{"title":"4. 选择微调方法","id":"4-选择微调方法","index":"4"},{"title":"5. LoRA 低成本微调","id":"5-LoRA-低成本微调","index":"5"},{"title":"6. QLoRA 低显存优化","id":"6-QLoRA-低显存优化","index":"6"},{"title":"7. 训练后推理","id":"7-训练后推理","index":"7"}]}