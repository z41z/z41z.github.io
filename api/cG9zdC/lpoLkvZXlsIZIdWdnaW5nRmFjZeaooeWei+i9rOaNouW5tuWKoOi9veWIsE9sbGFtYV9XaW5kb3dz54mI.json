{"title":"如何将 Hugging Face 模型转换并加载到 Ollama（Windows 版）","date":"2024-11-01T01:00:00.000Z","slug":"如何将HuggingFace模型转换并加载到Ollama_Windows版","comments":true,"tags":["Hugging Face","Ollama","人工智能","大模型","模型训练","模型转换"],"updated":"2025-09-25T01:08:14.182Z","content":"<h2 id=\"环境准备\">环境准备<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#环境准备\"></a></h2><h3 id=\"1-安装-Python-依赖\">1. 安装 Python 依赖<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#1-安装-Python-依赖\"></a></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install transformers torch</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-下载并编译-llama-cpp\">2. 下载并编译 llama.cpp<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#2-下载并编译-llama-cpp\"></a></h3><ul>\n<li><p><strong>方法 1（使用 Git）</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/ggerganov/llama.cpp.git</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>方法 2（直接下载 ZIP）</strong></p>\n<ul>\n<li><a href=\"https://github.com/ggerganov/llama.cpp/archive/refs/heads/master.zip\" target=\"_blank\" rel=\"noopener\">点击这里下载 llama.cpp</a></li>\n<li>解压后进入 <code>llama.cpp</code> 目录。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"3-安装-llama-cpp-依赖\">3. 安装 llama.cpp 依赖<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#3-安装-llama-cpp-依赖\"></a></h3><ul>\n<li><p><strong>WSL（Windows Subsystem for Linux）用户</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Windows 原生 Python 用户</strong><br>直接运行 <code>convert.py</code> 即可。</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"转换-Hugging-Face-模型为-GGUF\">转换 Hugging Face 模型为 GGUF<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#转换-Hugging-Face-模型为-GGUF\"></a></h2><h3 id=\"完整转换脚本（Windows-版）\">完整转换脚本（Windows 版）<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#完整转换脚本（Windows-版）\"></a></h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 指定 Hugging Face 模型名称（可以更改为你想要的模型）</span></span><br><span class=\"line\">MODEL_NAME = <span class=\"string\">\"mistralai/Mistral-7B-Instruct-v0.1\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 下载 Hugging Face 模型</span></span><br><span class=\"line\">print(<span class=\"string\">f\"Downloading model: <span class=\"subst\">&#123;MODEL_NAME&#125;</span>\"</span>)</span><br><span class=\"line\">model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=<span class=\"string\">\"auto\"</span>)</span><br><span class=\"line\">tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 保存模型到本地</span></span><br><span class=\"line\">MODEL_DIR = <span class=\"string\">\"hf_model\"</span></span><br><span class=\"line\">os.makedirs(MODEL_DIR, exist_ok=<span class=\"literal\">True</span>)</span><br><span class=\"line\">model.save_pretrained(MODEL_DIR)</span><br><span class=\"line\">tokenizer.save_pretrained(MODEL_DIR)</span><br><span class=\"line\">print(<span class=\"string\">f\"Model saved to <span class=\"subst\">&#123;MODEL_DIR&#125;</span>\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 转换为 GGUF 格式</span></span><br><span class=\"line\">GGUF_OUTPUT = <span class=\"string\">\"mistral-7b-instruct.Q4_K_M.gguf\"</span></span><br><span class=\"line\">CONVERT_SCRIPT = <span class=\"string\">\"llama.cpp/scripts/convert-hf-to-gguf.py\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">f\"Converting model to GGUF format: <span class=\"subst\">&#123;GGUF_OUTPUT&#125;</span>\"</span>)</span><br><span class=\"line\">os.system(<span class=\"string\">f\"python <span class=\"subst\">&#123;CONVERT_SCRIPT&#125;</span> <span class=\"subst\">&#123;MODEL_DIR&#125;</span> --outfile <span class=\"subst\">&#123;GGUF_OUTPUT&#125;</span> --format Q4_K_M\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"Conversion complete! You can now load the model into Ollama.\"</span>)</span><br></pre></td></tr></table></figure>\n<hr>\n<h2 id=\"加载到-Ollama\">加载到 Ollama<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#加载到-Ollama\"></a></h2><h3 id=\"1-创建-Ollama-Modelfile\">1. 创建 Ollama Modelfile<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#1-创建-Ollama-Modelfile\"></a></h3><p>在 <code>Modelfile</code> 文件中写入：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">FROM ./mistral-7b-instruct.Q4_K_M.gguf</span><br><span class=\"line\">PARAMETER temperature 0.7</span><br><span class=\"line\">SYSTEM &quot;You are a helpful AI assistant.&quot;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-加载到-Ollama\">2. 加载到 Ollama<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#2-加载到-Ollama\"></a></h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ollama create my-mistral -f Modelfile</span><br><span class=\"line\">ollama run my-mistral</span><br></pre></td></tr></table></figure>\n<hr>\n<h2 id=\"注意事项\">注意事项<a href=\"post/如何将HuggingFace模型转换并加载到Ollama_Windows版#注意事项\"></a></h2><ul>\n<li><strong>路径问题</strong>：如果 <code>llama.cpp</code> 目录路径带空格，Windows 可能会报错，建议放在 <code>C:\\llama.cpp</code> 目录下。</li>\n<li><strong>GGUF 格式</strong>：Ollama 只支持 <strong>GGUF</strong>，所以转换时一定要使用 <code>--format Q4_K_M</code>（或 <code>Q6_K</code>、<code>F16</code>）。</li>\n<li><strong>显存要求</strong>：不同量化格式对显存需求不同，例如 <code>Q4_K_M</code> 适用于 8GB 显存，<code>F16</code> 需要 32GB 以上。</li>\n</ul>\n","prev":{"title":"本地训练 DeepSeek 语言模型","slug":"本地训练DeepSeek语言模型"},"next":{"title":"Ollama Modelfile 规则说明","slug":"Ollama Modelfile 规则说明"},"link":"https://www.alipay.one/post/如何将HuggingFace模型转换并加载到Ollama_Windows版/","toc":[{"title":"环境准备","id":"环境准备","index":"1","children":[{"title":"1. 安装 Python 依赖","id":"1-安装-Python-依赖","index":"1.1"},{"title":"2. 下载并编译 llama.cpp","id":"2-下载并编译-llama-cpp","index":"1.2"},{"title":"3. 安装 llama.cpp 依赖","id":"3-安装-llama-cpp-依赖","index":"1.3"}]},{"title":"转换 Hugging Face 模型为 GGUF","id":"转换-Hugging-Face-模型为-GGUF","index":"2","children":[{"title":"完整转换脚本（Windows 版）","id":"完整转换脚本（Windows-版）","index":"2.1"}]},{"title":"加载到 Ollama","id":"加载到-Ollama","index":"3","children":[{"title":"1. 创建 Ollama Modelfile","id":"1-创建-Ollama-Modelfile","index":"3.1"},{"title":"2. 加载到 Ollama","id":"2-加载到-Ollama","index":"3.2"}]},{"title":"注意事项","id":"注意事项","index":"4"}]}